import cv2
import numpy as np
import os
from matplotlib import pyplot as plt
import time
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

mp_holistic = mp.solutions.holistic # Holistic model
mp_drawing = mp.solutions.drawing_utils # Drawing utilities
path_data = r"C:\Users\Hami H\Desktop\project\train"
labels = []

def extract_hand_keypoints(results):
    """
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„ holistic Ø±Ø§ Ú¯Ø±ÙØªÙ‡ØŒ
    Ù†Ù‚Ø§Ø· Ø¯Ø³Øª Ø±Ø§Ø³Øª Ùˆ Ú†Ù¾ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø±Ø¯Ù‡ØŒ
    Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¯Ø± Ù‚Ø§Ù„Ø¨ ÛŒÚ© Ø¢Ø±Ø§ÛŒÙ‡ numpy Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """

    # Ø§Ú¯Ø± Ø¯Ø³Øª Ø±Ø§Ø³Øª Ù¾ÛŒØ¯Ø§ Ø´Ø¯:
    if results.right_hand_landmarks:
        right_hand = np.array(
            [[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]
        ).flatten()
    else:
        # Ø§Ú¯Ø± Ø¯Ø³ØªÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯ØŒ Ø¨Ø§ ØµÙØ± Ù¾Ø± Ø´ÙˆØ¯
        right_hand = np.zeros(21 * 3)

    # Ø§Ú¯Ø± Ø¯Ø³Øª Ú†Ù¾ Ù¾ÛŒØ¯Ø§ Ø´Ø¯:
    if results.left_hand_landmarks:
        left_hand = np.array(
            [[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]
        ).flatten()
    else:
        left_hand = np.zeros(21 * 3)

    # Ø§Ø¯ØºØ§Ù… Ø¯Ùˆ Ø¯Ø³Øª Ø¯Ø± ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± (Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹ 126 Ø¹Ø¯Ø¯)
    features = np.concatenate([left_hand, right_hand])

    # --- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ---
    # Ú†ÙˆÙ† mediapipe Ù…Ø®ØªØµØ§Øª Ø±Ø§ Ø¨ÛŒÙ† 0 ØªØ§ 1 Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØµÙˆÛŒØ± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ØŒ
    # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ÙÙ‚Ø· Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒÙ… Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØµÙØ± Ùˆ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ÛŒÚ© Ø¯Ø§Ø±Ù†Ø¯.
    mean = np.mean(features)
    std = np.std(features) if np.std(features) != 0 else 1e-6
    normalized_features = (features - mean) / std

    return normalized_features

def data_to_array(path_data):
    global labels
    """
    Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡â€ŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ú¯Ø±ÙØªÙ‡ØŒ
    ØªÙ…Ø§Ù… ØªØµØ§ÙˆÛŒØ± Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø±Ø¯Ù‡ Ùˆ
    Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    all_data_X = []  # Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    all_data_Y = []

    with mp_holistic.Holistic(static_image_mode=True) as holistic:
        for folder in os.listdir(path_data):
            folder_path = os.path.join(path_data, folder)
            if not os.path.isdir(folder_path):
                continue
            if folder not in labels:
                labels.append(folder)

            for filename in os.listdir(folder_path):
                file_path = os.path.join(folder_path, filename)

                # Ø®ÙˆØ§Ù†Ø¯Ù† ØªØµÙˆÛŒØ±
                image = cv2.imread(file_path)
                if image is None:
                    continue  # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ ØªØµÙˆÛŒØ± Ù†Ø¨ÙˆØ¯ØŒ Ø±Ø¯ Ú©Ù†

                # RGB ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

                # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙˆØ³Ø· mediapipe
                results = holistic.process(image_rgb)

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
                features = extract_hand_keypoints(results)
                all_data_X.append(features)
                all_data_Y.append(labels.index(folder))

    return np.array(all_data_X), np.array(all_data_Y)


def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR-CONVERSION BGR-to-RGB
    image.flags.writeable = False                  # Convert image to not-writeable
    results = model.process(image)                 # Make prediction
    image.flags.writeable = True                   # Convert image to writeable 
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR-COVERSION RGB-to-BGR
    return image, results
def draw_landmarks(image, results):
    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections
    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections
    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections
    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections
def draw_styled_landmarks(image, results):
    # Draw face connections
    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, 
                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), 
                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)
                             ) 
    # Draw pose connections
    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), 
                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)
                             ) 
    # Draw left hand connections
    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, 
                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), 
                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)
                             ) 
    # Draw right hand connections  
    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, 
                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), 
                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)
                             )
dataX, dataY = data_to_array(path_data)
print(labels)

timesteps = 30
features = 126  # ÛŒØ§ 63 Ø§Ú¯Ø± ÙÙ‚Ø· ÛŒÚ© Ø¯Ø³Øª

sequences = []
labels_seq = []

for i in range(len(dataX) - timesteps + 1):
    sequences.append(dataX[i:i+timesteps])
    labels_seq.append(dataY[i+timesteps-1])  # Ø¨Ø±Ú†Ø³Ø¨ Ø¢Ø®Ø± ØªÙˆØ§Ù„ÛŒ

dataX_seq = np.array(sequences)
dataY_seq = np.array(labels_seq)
print(dataX_seq.shape, dataY_seq.shape)  # Ø¨Ø§ÛŒØ¯ (num_seq, 30, 126) Ùˆ (num_seq,) Ø¨Ø§Ø´Ø¯

model = Sequential([
    LSTM(128, return_sequences=True, activation='relu', input_shape=(timesteps, features)),
    LSTM(128, return_sequences=True, activation='relu'),
    LSTM(64, return_sequences=False, activation='relu'),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(len(labels), activation='softmax')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    dataX_seq, dataY_seq,
    epochs=150,
    batch_size=32,
    verbose=1
)

model.save("hand_model_optimized.h5")

cap = cv2.VideoCapture(0)
# Set mediapipe model 
with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    while cap.isOpened():

        # Read feed
        ret, frame = cap.read()

        # Make detections
        image, results = mediapipe_detection(frame, holistic)
        # Ø¯Ø³Øª Ú†Ù¾
        
        sequence_buffer = []
        
        features = extract_hand_keypoints(results)
        sequence_buffer.append(features)

        if len(sequence_buffer) > 30:
            sequence_buffer.pop(0)

        if len(sequence_buffer) == 30:
            input_seq = np.expand_dims(sequence_buffer, axis=0)
            prediction = model.predict(input_seq)
            pred_class = np.argmax(prediction)
            # ðŸ”¹ Ù†Ù…Ø§ÛŒØ´ Ù†ØªÛŒØ¬Ù‡ Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±
            cv2.putText(image, f'{labels[pred_class]}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
        
        # Draw landmarks
        draw_styled_landmarks(image, results)

        # Show to screen
        cv2.imshow('OpenCV Feed', image)

        # Break gracefully
        if cv2.getWindowProperty("OpenCV Feed", cv2.WND_PROP_VISIBLE) < 1:
            break
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break
    cap.release()
    cv2.destroyAllWindows()
draw_landmarks(frame, results)
cv2.imshow("result", cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
cv2.waitKey(0)
cv2.destroyAllWindows()